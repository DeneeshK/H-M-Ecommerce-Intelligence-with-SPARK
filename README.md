# ğŸ›ï¸ H&M E-commerce Intelligence Platform

> End-to-end Big Data & Machine Learning pipeline for customer analytics, personalized recommendations, and churn prediction using Apache Spark

[![Apache Spark](https://img.shields.io/badge/Apache%20Spark-3.5.0-E25A1C?style=flat&logo=apachespark)](https://spark.apache.org/)
[![Python](https://img.shields.io/badge/Python-3.8+-3776AB?style=flat&logo=python)](https://www.python.org/)
[![PySpark](https://img.shields.io/badge/PySpark-MLlib-orange)](https://spark.apache.org/mllib/)

---

##  Project Overview

A production-grade data engineering and machine learning system built to solve real e-commerce challenges using **31.8 million transactions** from H&M's fashion retail data.

### Business Problems Solved
1. **Customer Churn Prediction** - Identify at-risk customers before they leave
2. **Personalized Recommendations** - Increase conversion with AI-powered product suggestions  
3. **Customer Segmentation** - Enable targeted marketing campaigns

### Technical Highlights
-  Distributed ETL pipeline processing **3.5GB** of data using Apache Spark
-  Scalable feature engineering with **1.37M customers** and **105K products**
-  3 production-ready ML models (Churn, Recommendations, Segmentation)
-  End-to-end pipeline: Raw Data â†’ ETL â†’ ML â†’ Business Insights

---

## Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Raw Data      â”‚  31.8M transactions, 1.37M customers, 105K products
â”‚   (CSV files)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ETL Pipeline  â”‚  Spark-based data cleaning, validation, feature engineering
â”‚   (PySpark)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Feature Store  â”‚  Customer features (RFM, behavioral), Product features
â”‚   (Parquet)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ML Models     â”‚  Churn (GBT), Recommendations (ALS), Segmentation (K-Means)
â”‚  (Spark MLlib)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Predictions   â”‚  Business insights, dashboards, API endpoints
â”‚   & Insights    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

##  Project Structure
```
HM_Ecommerce_Project/
â”œâ”€â”€ raw_data/              # Original datasets (gitignored)
â”œâ”€â”€ processed_data/        # Cleaned & transformed data (gitignored)
â”œâ”€â”€ scripts/               # Production ETL & ML scripts
â”‚   â”œâ”€â”€ etl/
â”‚   â”‚   â”œâ”€â”€ extract.py
â”‚   â”‚   â”œâ”€â”€ transform.py
â”‚   â”‚   â””â”€â”€ load.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ churn_prediction.py
â”‚   â”‚   â”œâ”€â”€ recommendation.py
â”‚   â”‚   â””â”€â”€ segmentation.py
â”‚   â””â”€â”€ pipeline/
â”‚       â””â”€â”€ main_pipeline.py
â”œâ”€â”€ notebooks/             # Jupyter notebooks for EDA & experiments
â”‚   â”œâ”€â”€ 01_EDA.ipynb
â”‚   â”œâ”€â”€ 02_Feature_Engineering.ipynb
â”‚   â”œâ”€â”€ 03_Churn_Model.ipynb
â”‚   â”œâ”€â”€ 04_Recommendations.ipynb
â”‚   â””â”€â”€ 05_Segmentation.ipynb
â”œâ”€â”€ models/                # Saved ML models (gitignored)
â”œâ”€â”€ outputs/               # Results, reports, predictions (gitignored)
â”œâ”€â”€ logs/                  # Pipeline execution logs (gitignored)
â”œâ”€â”€ config/                # Configuration files
â”œâ”€â”€ DATA_DICTIONARY.md     # Dataset documentation
â””â”€â”€ README.md              # This file
```

---

##  Quick Start

### Prerequisites
- Python 3.8+
- Apache Spark 3.5.0
- 8GB+ RAM recommended

### Installation
```bash
# Clone repository
git clone https://github.com/YOUR_USERNAME/HM_Ecommerce_Project.git
cd HM_Ecommerce_Project

# Create virtual environment
conda create -n spark-env python=3.8
conda activate spark-env

# Install dependencies
pip install -r requirements.txt
```

### Run the Pipeline
```bash
# Execute end-to-end pipeline
python scripts/pipeline/main_pipeline.py --config config/config.yaml
```

---

##  Results & Business Impact

### Model Performance
| Model | Metric | Score |
|-------|--------|-------|
| **Churn Prediction** | AUC-ROC | TBD |
| **Recommendations** | Precision@10 | TBD |
| **Segmentation** | Silhouette Score | TBD |

### Business Impact (Projected)
-  **Churn Reduction:** X% decrease in customer attrition
-  **Revenue Lift:** Y% increase from personalized recommendations  
-  **Marketing Efficiency:** Z% improvement in campaign targeting

---

##  Tech Stack

- **Big Data Processing:** Apache Spark 3.5.0, PySpark
- **Machine Learning:** Spark MLlib, scikit-learn
- **Data Storage:** Parquet, CSV
- **Orchestration:** Python scripts
- **Notebooks:** Jupyter
- **Version Control:** Git

---

##  Dataset

**Source:** [Kaggle - H&M Personalized Fashion Recommendations](https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations)

- **Transactions:** 31,788,324 records (2018-2020)
- **Customers:** 1,371,980 profiles
- **Products:** 105,542 articles with 25 attributes

---

##  Development Timeline

| Week | Phase | Deliverables |
|------|-------|--------------|
| 1-3 | ETL Pipeline | Data extraction, cleaning, feature engineering |
| 4-7 | ML Models | Churn, recommendations, segmentation models |
| 8 | Integration | Batch predictions, dashboard, API |
| 9 | Documentation | Technical docs, business report, presentation |

---

##  Author

**Deneesh Kumar**
- LinkedIn: 
- Email: dksteam2004@gmail.com



##  Acknowledgments

- H&M Group for providing the dataset
- Apache Spark community for excellent documentation
- Kaggle for hosting the competition
